- The obvious impact of Docker and the ease of use it brings to Linux containers is the possibility to redefine the organizational divide between business, application devel‐ opment, and IT infrastructure teams. In a sense, Docker provides a tangible technol‐ ogy for implementing DevOps, which is the merger (or at least an armistice) between the often competing teams of development and operations. Containerization mod‐ ernizes IT environments and, at an organizational level, allows for “proper” owner‐ ship of the technology stack and processes, reducing handovers and the costly change coordination that comes with them
- Docker’s role as both a packaging format for the application and a unifying interface and methodology enables the application team to own the Docker-formatted con‐ tainer image, including all dependencies, while allowing operations to retain infra‐ structure ownership. With a standardized container infrastructure in place, the IT organization can then focus on building and managing deployments, meeting their security standards, automation needs, skill levels and ultimately cost profile, all without losing the ability to hold the application team accountable for the security and cost impact of their code that is deployed inside the container.
- Docker also brings with it greater efficiencies of scale and performance—by shrinking application footprints through Docker-formatted containers, system-level dependen‐ cies are reduced to a bare minimum, often dozens-to-hundreds of megabytes in size. Compare this to traditional virtual machine images, which typically consume giga‐ bytes of storage...but when you factor in performance, it goes beyond simply being innovative and becomes truly disruptive.
- But perhaps the greatest innovation and most significant impact delivered by Docker and Linux containers is the fundamental change to application consumption. The monolithic application stack as we know it can be broken into dozens or even hun‐ dreds of tiny, single-minded applications that, when woven together, perform the same function as the traditional application. The benefit, however, is that these pieces can be rewritten, reused, and managed far more efficiently than monolithic applica‐ tions, delivering a truly composite application built entirely of microservices.
- Containers represent the way forward for the application development world, but it’s critical that we do not lose sight of the old as we bring in the new. Docker and Linux containers are not without challenges. Management, security, and certification are three of the most glaring challenges to enterprise adoption, and these concerns are not so dissimilar from more traditional applications. Obviously, containers must be deployed on a secure host, but, more importantly, container security will be defined by what is in a given container—is it free of vulnerabilities, malware, and known exploits? Having the appropriate signature on a given containerized application, from a trusted, certified source goes a long way towards effectively answering these ques‐ tions.
- Docker is a tool that promises to easily encapsulate the process of creating a distribut‐ able artifact for any application, deploying it at scale into any environment, and streamlining the workflow and responsiveness of agile software organizations.
- 1. Developers build the Docker image and ship it to the registry.
2. Operations engineers provide configuration details to the container and provi‐ sion resources.
3. Developers trigger deployment.
- Docker is a powerful technology, and that often means something that comes with a high level of complexity. But the fundamental architecture of Docker is a simple cli‐ ent/server model, with only one executable that acts as both components, depending on how you invoke the docker command. Underneath this simple exterior, Docker heavily leverages kernel mechanisms such as iptables, virtual bridging, cgroups, namespaces, and various filesystem drivers.
- Docker consists of at least two parts: the client and the server/daemon (see Figure 2-3). Optionally there is a third component called the registry, which stores Docker images and metadata about those images. The server does the ongoing work of running and managing your containers, and you use the client to tell the server what to do. The Docker daemon can run on any number of servers in the infrastruc‐ ture, and a single client can address any number of servers. Clients drive all of the communication, but Docker servers can talk directly to image registries when told to do so by the client. Clients are responsible for directing servers what to do, and servers focus on hosting containerized applications.
- The Docker server acts as a virtual bridge and the containers are clients behind it. A bridge is just a network device that repeats traffic from one side to another. So you can think of it like a mini virtual net‐ work with hosts attached.
- To begin with, Docker’s architecture aims it squarely at applications that are either stateless or where the state is externalized into data stores like databases or caches.
- But this means that doing things like putting a data‐ base engine inside Docker is basically like trying to swim against the current. It’s not that you can’t do it, or even that you shouldn’t do it; it’s just that this is not the most obvious use case for Docker and if it’s the one you start with, you may find yourself disappointed early on. Some good applications for Docker include web frontends, backend APIs, and short-running tasks, like maintenance scripts that might normally be handled by cron.
- A good way to start shaping your understanding of how to leverage Docker is to think of containers not as virtual machines, but as very lightweight wrappers around a single Unix process. During actual implementation, that process might spawn oth‐ ers, but on the other hand, one statically compiled binary could be all that’s inside your container (see “Outside Dependencies” on page 111 for more information). Containers are also ephemeral: they may come and go much more readily than a vir‐ tual machine.
- Containers are isolated from each other, but it’s probably more limited than you might expect. While you can put limits on their resources, the default container con‐ figuration just has them all sharing CPU and memory on the host system, much as you would expect from colocated Unix processes. This means that unless you con‐ strain them, containers can compete for resources on your production machines. That is sometimes what you want, but it impacts your design decisions. Limits on CPU and memory use are possible through Docker but, in most cases, they are not the default like they would be from a virtual machine.
- Containerized processes are also just processes on the Docker server itself. They are running on the same exact instance of the Linux kernel as the host operating system. They even show up in the ps output on the Docker server. That is utterly different from a hypervisor where the depth of process isolation usually includes running an entirely separate instance of the operating system for each virtual machine.
- A good example of the kind of application that containerizes well is a web application that keeps its state in a database. You might also run something like ephemeral memcache instances in containers. If you think about your web application, though, it probably has local state that you rely on, like configuration files. That might not seem like a lot of state, but it means that you’ve limited the reusability of your con‐ tainer, and made it more challenging to deploy into different environments, without maintaining configuration data in your codebase.
- In many cases, the process of containerizing your application means that you move configuration state into environment variables that can be passed to your application from the container. This allows you to easily do things like use the same container to run in either production or staging environments. In most companies, those environ‐ ments would require many different configuration settings, from the names of data‐ bases to the hostnames of other service dependencies.
- Databases are often where scaled applications store state, and nothing in Docker interferes with doing that for containerized applications. Applications that need to store files, however, face some challenges. Storing things to the container’s filesystem will not perform well, will be extremely limited by space, and will not preserve state across a container lifecycle. Applications that need to store filesystem state should be considered carefully before putting them into Docker.
- uild, you only have to rebuild the layers that include and build upon the change you’re deploying. This saves time and bandwidth because containers are shipped around as layers and you don’t have to ship layers that a server already has stored.
- The Docker command-line tool contains a build flag that will consume a Dockerfile and produce a Docker image. Each command in a Dockerfile generates a new layer in the image, so it’s easy to reason about what the build is going to do by looking at the Dockerfile itself. The great part of all of this standardization is that any engineer who has worked with a Dockerfile can dive right in and modify the build of any other application. Because the Docker image is a standardized artifact, all of the tooling behind the build will be the same regardless of the language being used, the OS distri‐ bution it’s based on, or the number of layers needed.
- Docker server
The docker command run in daemon mode. This turns a Linux system into a Docker server that can have containers deployed, launched, and torn down via a remote client.
Docker images
Docker images consist of one or more filesystem layers and some important met‐ adata that represent all the files required to run a Dockerized application. A sin‐ gle Docker image can be copied to numerous hosts. A container will typically have both a name and a tag. The tag is generally used to identify a particular release of an image.
Docker container
A Docker container is a Linux container that has been instantiated from a Docker image. A specific container can only exist once; however, you can easily create multiple containers from the same image.
Atomic host
An atomic host is a small, finely tuned operating system image, like CoreOS and Project Atomic, that supports container hosting and atomic OS upgrades.